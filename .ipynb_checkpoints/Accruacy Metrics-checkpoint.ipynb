{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement : How many asteroids will hit the earth?\n",
    "\n",
    "1 million occurences - 0.1% will hit \n",
    "\n",
    "No asteroid is going to hit - 99.9% accuracy\n",
    "\n",
    "Is it a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "<img src=\"img/cm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy = (TP + TN)/(TP + TN + FP + FN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use Accuracy?**\n",
    "\n",
    "* When there is no imbalance in classes\n",
    "    * Hits the earth / Doesn't hit the earth\n",
    "    * 10000 rows\n",
    "        * Hits the earth - 10\n",
    "        * Doesn't hit the earth - 9990\n",
    "        * Accuracy as a metric is not a good choice\n",
    "    * Image Classification (1000 images - 500 dogs & 500 cats)\n",
    "        * Accuracy as a metric is a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "* What proportion of predicted positives is actually positive?\n",
    "\n",
    "**Precision = TP / (TP + FP)**\n",
    "\n",
    "In asteroid problem, we never predicted TP. So, precision is 0.\n",
    "\n",
    "\n",
    "**When to use precision?**\n",
    "\n",
    "* Precision is a valid choice of evaluation metrics when we want to be very sure of our prediction. e.g. if we are building a system which is used to predict decrease in credit limit of a customer then we will want to be very sure about the prediction or it may result in customer dissatisfaction.\n",
    "\n",
    "**Problem** - If we make our model very precise, then we risk a chance of leaving a lot of credit defaulters hence losing money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall**\n",
    "\n",
    "What proportion of actual positives is correctly classified?\n",
    "\n",
    "**Recall = TP / (TP + FN)**\n",
    "\n",
    "In asteroid prediction problem no true positive, so Recall = 0.\n",
    "\n",
    "**When to use recall?**\n",
    "\n",
    "* Recall is a correct choice of evaluation metric when we want to capture as many positives as possible. e.g. if we are building a system to predict if an asteroid will hit the earth or not, we want to capture the hit even if we are not very sure.\n",
    "\n",
    "**Problem** - If we predict 1 for all rows of test data, then recall is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Score**\n",
    "\n",
    "F1 score is a number between 0 and 1. It is the harmonic mean of precision and recall.\n",
    "\n",
    "**F1 = 2 * (precision * recall) / (precision + recall)**\n",
    "\n",
    "**When to use F1?**\n",
    "\n",
    "We want to have a model which has good precision and good recall.\n",
    "\n",
    "F1 score maintains balance b/w precision and recall.\n",
    "\n",
    "If precision is low, and recall is low, then F1 is also low.\n",
    "\n",
    "If you're in NCB catching bollywood drug takers, you want to be sure that the person is a criminal (Precision), but you also want to be sure that you catch as many criminals as possible (Recall). F1 score will help you balance these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.6666666666666665\n",
      "Precision Score : 0.6\n",
      "Recall Score : 0.75\n",
      "Accuracy Score : 0.625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score,precision_score,recall_score\n",
    "\n",
    "y_actual = [0,1,1,1,0,1,1,0]\n",
    "y_pred =   [0,0,1,1,0,1,0,1]\n",
    "\n",
    "print(f'F1 Score : {f1_score(y_pred,y_actual)}')\n",
    "print(f'Precision Score : {precision_score(y_pred,y_actual)}')\n",
    "print(f'Recall Score : {recall_score(y_pred,y_actual)}')\n",
    "print(f'Accuracy Score : {accuracy_score(y_pred,y_actual)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/diabetes.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pregnancies','Insulin','BMI']\n",
    "\n",
    "X = df[features]\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668    0\n",
       "324    0\n",
       "624    0\n",
       "690    0\n",
       "473    0\n",
       "      ..\n",
       "355    1\n",
       "534    0\n",
       "344    0\n",
       "296    1\n",
       "462    0\n",
       "Name: Outcome, Length: 154, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6688311688311688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,precision_score,recall_score,f1_score\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Accuracy - we go with majority class and substitute it as the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_accuracy = df['Outcome'].value_counts()[0]/768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The null accuracy of this problem is : 65.10416666666666%\n"
     ]
    }
   ],
   "source": [
    "print(f'The null accuracy of this problem is : {null_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score: 0.6688311688311688\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[85 14]\n",
      " [37 18]]\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.77        99\n",
      "           1       0.56      0.33      0.41        55\n",
      "\n",
      "    accuracy                           0.67       154\n",
      "   macro avg       0.63      0.59      0.59       154\n",
      "weighted avg       0.65      0.67      0.64       154\n",
      "\n",
      "F1 Score : 0.41379310344827586\n",
      "Precision Score : 0.32727272727272727\n",
      "Recall Score : 0.5625\n",
      "Accuracy Score : 0.6688311688311688\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Accuracy Score: {accuracy_score(y_test,y_pred)}')\n",
    "print(f'\\n\\nConfusion Matrix:\\n {confusion_matrix(y_test,y_pred)}')\n",
    "print(f'\\n\\nClassification Report:\\n {classification_report(y_test,y_pred)}')\n",
    "\n",
    "print(f'F1 Score : {f1_score(y_pred,y_test)}')\n",
    "print(f'Precision Score : {precision_score(y_pred,y_test)}')\n",
    "print(f'Recall Score : {recall_score(y_pred,y_test)}')\n",
    "print(f'Accuracy Score : {accuracy_score(y_pred,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity or Recall or True Positive Rate\n",
    "\n",
    "#### Specificity - when actual value is -ve and we try to determine how often the prediction is correct?\n",
    "**Specificity = TN / (TN + FP)**\n",
    "\n",
    "https://science.thewire.in/health/icmr-antigen-tests-advisory-negative-results-rt-pcr-retest/\n",
    "\n",
    "\n",
    "ICMR said in its advisory that it had evaluated the antigen kit’s performance in two labs, and found its sensitivity to be 50.6% and 84%, and specificity 99.3% and 100%, respectively. Given the high specificity, the council said, the kit could be used in two scenarios. The first is to diagnose people with influenza-like illnesses and asymptomatic contacts of patients in COVID-19 containment zones – areas with clusters of cases where testing is typically intensified. The second scenario is for the diagnosis of both symptomatic and asymptomatic people in all healthcare facilities – whether in containment zones or outside them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 100% specificity would mean that the test will never confuse other antibodies for Novel coronavirus.\n",
    "\n",
    "A 98% sensitivity will mean if test kit checks 100 samples, 98% of time it will detect antibodies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC - Area under ROC curve\n",
    "\n",
    "#### ROC - Receiver Operating Characteristics curve\n",
    "\n",
    "\n",
    "**AUC ROC determines how well the probabilities from the positive classes are separated from the negative classes.**\n",
    "\n",
    "ROC is a curve between True Positive Rate i.e. Sensitivity and False Positive Rate i.e. 1-Specificity.\n",
    "\n",
    "* FPR = FP / (FP+TN)\n",
    "* TPR = TP / (TP+FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Job !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
